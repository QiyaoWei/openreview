{
    "2023": {
        "main": "manipulation:24;reinforcement learning:22;imitation learning:17;robotic manipulation:8;transformer:8;learning from demonstration:7;large language model:7;autonomous driving:6;representation learning:6;task and motion planning:6;dexterous manipulation:6;robot manipulation:6;human-robot interaction:6;diffusion model:5;model-based reinforcement learning:5;robot learning:5;deformable object manipulation:5;offline reinforcement learning:4;contact-rich manipulation:4;sim-to-real transfer:4;diffusion:4;planning:4;robotic:4;sim2real:4;sensor fusion:3;self-driving:3;generative model:3;graph neural network:3;generalization:3;multi-agent reinforcement learning:3;locomotion:3;navigation:3;multimodal learning:2;temporal reasoning:2;robustness:2;optimal control:2;behavior cloning:2;adversarial robustness:2;dataset:2;skill learning:2;legged locomotion:2;safety:2;symmetry:2;3d object detection:2;vision:2;trajectory prediction:2;energy-based model:2;autonomous vehicle:2;vision-based navigation:2;instruction following:2;visual navigation:2;active learning:2;adversarial attack:2;manipulation planning:2;language grounding:2;deformable manipulation:2;human-robot collaboration:2;mobile manipulation:2;dynamical system:2;natural language:2;semantic:2;large language model (llm):2;safe learning:2;motion planning:2;mechanical search:2;object rearrangement:2;multi-task learning:2;offline rl:2;long-horizon manipulation:2;3d manipulation:2;optimization:2;machine learning:2;pre-training:2;model learning:2;assistive robot:2;bimanual manipulation:2;differentiable filter:1;panoptic segmentation:1;inverse reinforcement learning:1;bayesian inference:1;automatic design tool:1;root-cause failure analysis:1;optimization-as-inference:1;command following:1;multimodal representation:1;human-in-the-loop:1;soft robot:1;non-parametric modelling:1;robotic control:1;stability:1;monocular depth estimation:1;camera perception:1;distillation:1;demonstration collection:1;augmented reality:1;manipulating personalized object:1;dataset collection:1;discretization:1;robot skill learning:1;closed-loop simulation:1;grasp synthesis:1;neural sdf:1;next-best-view planning:1;visual representation learning:1;reset-free reinforcement learning:1;learning from human feedback:1;multi-agent perception:1;multi-modal fusion:1;vehicle-to-everything (v2x) application:1;extrinsic calibration:1;differentiable optimization:1;jumping:1;visual-language guided policy:1;safety-critical scenario generation:1;adversarial training:1;end-to-end driving:1;intrinsic reward:1;learning skill:1;cold diffusion:1;part decomposition:1;constraint satisfaction problem:1;deep reinforcement learning:1;mapless navigation:1;context-aware decision-making:1;open-vocabulary semantic:1;scene graph:1;object grounding:1;continual learning:1;metaworld:1;unsupervised learning:1;contrastive learning:1;noise-contrastive estimation:1;model generalization:1;curiosity:1;wheeled-legged robot:1;quadrotor:1;adaptive control:1;learning from video:1;temporal modeling:1;learning from heterogeneous demonstration:1;network distillation:1;offline imitation learning:1;suction grasping:1;deep learning:1;learning-aware safety analysis:1;active information gathering:1;adversarial reinforcement learning:1;tactile:1;dexterity:1;functional grasping:1;model-based robot learning:1;differentiable physics-based simulation and rendering:1;neural field:1;foundation model:1;scene understanding:1;bimanual dexterous manipulation:1;multi-agent:1;autonomous racing:1;compliance control:1;lifelong learning:1;feedback control:1;quadrupedal locomotion:1;multi-agent interaction:1;game-theoretic motion planning:1;movement primitive:1;manifold:1;lfd:1;equivariance:1;partial observability:1;equivariant learning:1;offroad driving:1;few-shot imitation learning:1;planning under uncertainty:1;score matching:1;object disambiguation:1;language interaction:1;robotic learning:1;fine-tuning:1;real-world robotic:1;cloud robotic:1;robotic perception:1;articulated object manipulation:1;neural radiance field:1;in-hand object rotation:1;tactile sensing:1;visuotactile manipulation:1;heterogeneity:1;multi-robot teaming:1;simulation:1;multi-embodiment:1;dexterous grasping:1;planning with gesture:1;llm reasoning:1;robot navigation:1;formal method:1;action representation:1;reinforcement learning with 3d vision:1;non-prehensile manipulation:1;state estimation:1;human-object interaction:1;human intention:1;gaussian process regression:1;random feature:1;motion primitive:1;robot learning and planning:1;benchmark:1;neural sde:1;physics-informed learning:1;data-driven modeling:1;control:1;teleoperation:1;fleet learning:1;contact perception:1;vision-based:1;offline policy learning:1;positive unlabeled learning:1;behavioural cloning:1;semantic manipulation:1;keypoint perception:1;auto-labelling:1;offboard perception:1;trajectory refinement:1;content generation:1;nerf:1;grasping:1;low-level skill learning:1;path planning:1;collision avoidance:1;learned collision function:1;traffic simulation:1;multi-agent diffusion:1;language-guided robot grasping:1;referring grasp synthesis:1;visual grounding:1;in-context learning:1;language for robotic:1;learning for tamp:1;abstraction learning:1;long-horizon problem:1;object-centric representation:1;reward learning:1;semidefinite programming:1;traf\ufb01c simulation:1;contact modeling and manipulation:1;imitation from observation:1;tool use:1;design:1;preference learning:1;global-scale autonomous driving:1;prehensile and non-prehensile manipulation:1;object manipulation:1;pick and place:1;online fine-tuning:1;model-learning:1;task specification:1;collaborative manipulation:1;forecasting:1;model predictive control:1;policy evaluation:1;robot validation:1;interpretability:1;disentangled representation:1;neural policy:1;learning from human:1;rule-based planning:1;learning for manipulation:1;brain-robot interface:1;language model:1;distributed control:1;control barrier function:1;open-vocabulary:1;3d instance retrieval:1;koopman operator:1;one-shot imitation learning:1;unseen object pose estimation:1;shape warping:1;regret:1;online:1;learning:1;convex:1;obstacle:1;online adaptation:1;behavior prediction:1;visuomotor representation:1;robot collision:1;collision distance:1;data-driven simulation:1;learning from play:1;language-driven robotic:1;3d point cloud:1;language-guided policy:1;robot transfer learning:1;policy stitching:1;vision-based manipulation:1;multi-robot generalization:1;robot co-design:1;modular soft robot:1;tactile perception:1;needle threading:1;graph-based neural dynamic:1;multi-object interaction:1;proactive robot assistance:1;user routine understanding:1;interactive clarification:1;pomdp:1;online planning:1;guided search:1;preference-based learning:1;model-building:1;self-supervision:1;q-learning:1;sample-efficient rl:1;explainable ai:1;task planning:1;vision-language model:1;multi-view:1;planning as inference:1;variational inference:1;nonparametric learning:1;robotic assembly:1;pose estimation:1;3d perception:1;soft robotic:1;dynamic and control:1;monocular 3d object detection:1;long-horizon planning:1;tool usage:1;high-dimensional control:1;bi-manual dexterity:1;self-supervised:1;sensorimotor:1;agile locomotion:1;end-to-end vision-based control:1;sim-to-real:1;language-based planning:1;uncertainty estimation:1;conformal prediction:1;continuous control:1;skill discovery:1;causal learning:1;food manipulation:1;robot scooping:1;active perception:1;language-based robotic:1;learning from experience:1;unseen object instance segmentation:1;unsupervised multi object tracking:1;zero-shot:1;discrete frame:1;human-in-the-loop learning:1;preference-based rl:1;rlhf:1;robot task planning:1;semantic search:1;llm-based planning:1;3d scene graph:1;quadrupedal robot:1;gaussian process:1;vehicle dynamic:1;deep kernel learning:1;map construction:1;multi-view perception:1;long-range perception:1;language:1;seeing-eye robot:1;robotic guide dog:1;quadruped locomotion:1;autonomous:1;reset-free:1;vision and language model in robotic:1;object search:1;multi-modality:1;point cloud:1;relation:1;system identification:1;dynamic learning:1;reconnaissance:1;adversarial search:1;multi-robot:1;environment prediction:1;probabilistic inference:1;rearrangement planning:1;graph attention:1;scenario generation:1;quality diversity:1;learning abstraction:1;tactile control:1;high-resolution tactile sensor:1;stable dynamical system:1;reactive motion policy:1;task parametrization:1;task generalization:1;learning and control:1;koopman-based control:1;represention learning:1;contextual navigation:1;dynamic manipulation:1;self supervised learning:1;audio:1;normalizing flow:1;out-of-distribution detection:1;robotic introspection:1;assistive robotic:1;coverage:1;testing:1;counterfactual reasoning:1;autonomous navigation:1;interaction modeling:1;legged robotic:1;bayesian optimization:1;controller tuning:1;garment folding:1;model-based planning:1;waypoint:1;long-horizon:1;sim-to-real gap:1;causal discovery:1;cross-embodiement:1"
    },
    "2022": {
        "main": "reinforcement learning:25;imitation learning:21;autonomous driving:13;manipulation:11;robot manipulation:7;representation learning:7;robotic manipulation:6;transformer:6;meta-learning:5;human-robot interaction:5;transfer learning:5;deep reinforcement learning:5;dexterous manipulation:5;sim-to-real:5;task and motion planning:5;learning from demonstration:5;model predictive control:5;trajectory prediction:5;self-supervised learning:5;mobile manipulation:4;autonomous vehicle:4;motion forecasting:4;planning:4;robotic:4;offline reinforcement learning:4;safety:4;self-driving:3;locomotion:3;pose estimation:3;legged locomotion:3;point cloud:3;human robot interaction:3;inverse reinforcement learning:3;visual navigation:3;robot learning:3;deformable object manipulation:3;skill learning:3;motion planning:3;preference learning:2;simulation:2;planning under uncertainty:2;gaussian process:2;3d reconstruction:2;contrastive learning:2;imitation:2;model-based reinforcement learning:2;world model:2;deep learning:2;large language model:2;grounding model:2;semantic segmentation:2;differentiable simulation:2;curriculum learning:2;embodied ai:2;instruction following:2;interactive learning:2;multi-task learning:2;rl:2;grasp detection:2;graph network:2;learning from human demonstration:2;perception:2;multimodal learning:2;assistive robotic:2;distribution shift:2;forecasting:2;partial observability:2;legged robot:2;graph neural network:2;computer vision:2;generative model:2;learning-based control:2;neural radiance field:2;neural implicit representation:2;contact-rich manipulation:2;dynamical system:2;data augmentation:2;tool manipulation:2;control lyapunov function:2;rearrangement:2;tactile control:2;motion control:2;one-shot:2;model learning:2;differentiable rendering:2;navigation:2;neural field:2;sim2real:2;task specification:2;object-centric representation:2;human guidance:1;evaluative feedback:1;adversarial imitation learning (ail):1;actor-critic (ac):1;actor residual critic (arc):1;model adaptation:1;online robot control and prediction:1;embodied ai benchmark:1;everyday activity:1;probabilistic programming:1;differentiable physics:1;bayesian inference:1;causal inference:1;single-episode bayesian reinforcement learning:1;scene understanding:1;learning environment:1;cad model:1;sensor simulation:1;deep rl:1;informative path planning:1;context-aware decision-making:1;3d multi-object tracking:1;cross-camera fusion:1;robot teaching:1;causal generative model:1;scenario generation:1;bev map understanding:1;vehicle-to-vehicle (v2v) application:1;multi-agent reinforcement learning:1;interpretable machine learning:1;decision transformer:1;koopman:1;learning:1;model-predictive control:1;optimal control:1;sample-efficiency:1;movement primitive:1;episode-based:1;black-box:1;trust region:1;rotation:1;relative-supervision:1;self-supervision:1;modified rodrigue parameter:1;stereographic projection:1;whole-body control:1;visual demonstration:1;incorrect demonstration:1;policy learning:1;differentiable algorithm:1;control:1;offline imitation learning:1;model-based learning:1;reward learning:1;unsupervised domain adaptation:1;domain generalization:1;autonomous rl:1;offline rl:1;reset-free manipulation:1;tactile simulation:1;tactile manipulation:1;continuos control:1;evolution strategy:1;interactive imitation learning:1;active demonstration elicitation:1;embodied concept learner:1;nerf:1;grasping:1;transparent object:1;speed:1;sample-efficient reinforcement learning:1;expert intervention:1;option:1;planning with primitive:1;personalized learning:1;learning from heterogeneous demonstration:1;human-in-the-loop:1;fleet learning:1;coordinate frame:1;3d:1;grasp metric:1;grasp dataset:1;automatic annotation:1;robot design automation:1;latent optimization:1;graph grammar:1;category-level pose estimation:1;shape estimation:1;third-person video:1;contact:1;rigid body dynamic:1;robot navigation:1;heavy tailed policy:1;autonomous   driving:1;human   pose:1;key   point:1;skeletal representation:1;hri:1;group dynamic:1;hypernetwork:1;reinforcement:1;meta:1;meta-rl:1;meta-world:1;active learning:1;tactile pose sensing:1;in hand manipulation:1;dexterous in-hand manipulation:1;object rotation:1;policy search:1;approximate inference:1;versatile skill learning:1;distribution matching:1;multi-modal control:1;language instruction:1;out-of-distribution detection:1;epistemic uncertainty estimation:1;plastic bag manipulation:1;interactive prediction:1;integrating planning and learning:1;language model:1;vision-based navigation:1;bird \u2019s eye view semantic segmentation:1;encoder-decoder transformer:1;laplace approximation:1;epistemic uncertainty:1;3d object detection:1;image-goal navigation:1;perspective-n-point:1;ai habitat:1;adversarial:1;bimanual manipulation:1;food acquisition:1;robot-assisted feeding:1;multi-agent navigation:1;bin-picking:1;dexterous grasping:1;grasp planning:1;bilevel optimization:1;generalized policy learning:1;affordance model:1;goal-conditioned rl:1;vio:1;interpretable learning:1;camera calibration:1;performer:1;highly-constrained navigation:1;social navigation:1;dynamic model learning:1;visual prediction:1;acl:1;hyper-net:1;multi-objective curriculum:1;neuro-symbolic:1;hybrid force-velocity controller:1;precondition learning:1;assistive robot:1;ood generalization:1;riemannian manifold:1;motion learning:1;neural ode:1;semantic region prediction:1;egocentric vision:1;driver intent:1;risk object identification:1;normalizing flow:1;semantic perception:1;hierarchical control:1;temporally extended skill learning:1;hierarchical reinforcement learning:1;diverse skill learning:1;augmented reality:1;multi-robot system:1;assistive feeding:1;deformable manipulation:1;multisensory learning:1;multi-modal dynamic learning:1;heuristic learning:1;supervised leanring:1;extrinsic dexterity:1;actuator morphology:1;morphological computation:1;vision:1;walking:1;fully observable expert:1;language for robotic:1;lidar:1;shapley value:1;feature selection:1;depth completion:1;stability:1;robustness:1;deformable tactile sensor:1;visual model-based rl:1;object pose estimation:1;bayesian optimization:1;tactile perception:1;localization:1;3d deep learning:1;language-conditioned learning:1;attention:1;modularity:1;end-to-end learning:1;multi-robot perception:1;scene completion:1;robot grasping:1;grasp transfer:1;grasping contact modeling:1;6d object pose estimation:1;equivariance:1;affordance:1;correspondence:1;online learning:1;aerial robotic:1;observation model:1;maximum entropy:1;out-of-dynamic imitation learning:1;human play data:1;object affordance learning:1;slam:1;bayes filter:1;uncertainty:1;particle filtering:1;language grounding:1;behavior cloning:1;explainability:1;occluded environment:1;semantic scene understanding:1;long horizon planning:1;6d pose estimation:1;object-relative localization:1;proactive robot assistance:1;spatio-temporal object tracking:1;slip control:1;quadrotor trajectory tracking:1;visual representation learning:1;risk measure:1;implicit shape representation:1;reconstruction:1;data compression:1;contact-rich simulation:1;teleoperation:1;visual representation:1;robot experimentation:1;physical property:1;personalization:1;sparse reward reinforcement learning:1;learn from demonstration:1;task mismatch:1;object detection:1;point cloud sequence:1;residual reinforcement learning:1;global optimization:1;deformable object:1;visual-based navigation:1;learning from video:1;video demonstration dataset:1;real2sim:1;self-supervised reward learning:1;robotic simulation benchmark:1;adversarial attack:1;certifiable   robustness:1;camera motion perturbation:1;robotic perception:1;pushing dynamic learning:1;pushing manipulation:1;symmetry and equivariance:1;place recognition:1;se(3)-invariant:1;equivariant representation learning:1;3d point cloud:1;relation:1;object rearrangement:1;safe control:1;input limit:1;prosthesis:1;sensor fusion:1;multisensory perception:1;3d scene understanding:1;out-of-domain generalization:1;language:1;skill dynamic model:1;social preference:1;multi-agent:1;imaginary exploration:1;optimistic initialization:1;self-supervised depth estimation:1;multi-camera perception:1;structure-from-motion:1;3d learning:1;image translation:1;human-in-the-loop robot learning:1;run-time monitoring:1;formal method:1;linear temporal logic:1;certifiable imitation learning:1;flow:1;deformables:1;landmark-based navigation:1;incremental topological memory:1;camera-based tactile sensing:1;cross-modal tactile datum generation:1;autonomous prediction:1;long-tailed 3d detection:1;multimodal fusion:1;bin packing:1;point-cloud representation:1;scale balance:1;knowledge distillation:1;interactive perception:1;hierarchical imitation learning:1;meta learning:1;task planning:1;prompt:1;preference:1;multi-goal reinforcement learning:1;reinforcement learning theory:1;multimodal representation learning:1;path following:1;trajectory tracking:1;plan execution:1;verified autonomy:1;large scale robotic:1;polar rasterization:1;surface estimation:1;bev segmentation:1;multi-modal learning:1;cloth manipulation:1;contact point detection:1;7-dof grasping:1;general object grasping:1;all day depth estimation:1;photometric loss:1;obstacle avoidance:1;object representation:1"
    },
    "2021": {
        "main": "reinforcement learning:24;imitation learning:16;manipulation:12;robot learning:7;sim-to-real:6;deep learning:6;learning from demonstration:5;autonomous driving:5;offline reinforcement learning:5;deep reinforcement learning:5;robot manipulation:5;self-supervised learning:5;graph neural network:4;active learning:4;legged robot:4;offline rl:3;robotic:3;safety:3;mobile manipulation:3;model-based reinforcement learning:3;self-driving:3;human-robot interaction:3;vision:3;hri:3;exploration:3;computer vision for robotic application:2;neural radiance field:2;robotic manipulation:2;visual learning:2;representation learning:2;instruction following:2;dexterous manipulation:2;adversarial learning:2;human-in-the-loop:2;embodied ai:2;rrt:2;task and motion planning:2;multi-agent system:2;transparent object:2;computer vision:2;articulated object:2;model learning:2;deformable object manipulation:2;cloth manipulation:2;legged locomotion:2;hierarchical control:2;self-supervision:2;autonomous vehicle:2;riemannian manifold:2;implicit model:2;multi-agent interaction:2;benchmark:2;data augmentation:2;planning:2;dynamic model learning:2;reward learning:2;locomotion:2;learning control:2;sample efficiency:2;model-based rl:2;safe reinforcement learning:2;navigation:2;object-centric representation:2;generative model:2;deep learning in grasping and manipulation:1;learning-based dynamic modeling:1;3d-aware representation learning:1;constrained rl:1;multi-objective rl:1;deep rl:1;differentiable optimization:1;vision and language:1;spatial representation:1;semantic mapping:1;in-hand manipulation:1;object reorientation:1;workflow:1;no online tuning:1;self-supervised:1;monocular:1;depth prediction:1;sparse lidar:1;long-horizon manipulation:1;skill chaining:1;adversarial imitation learning:1;optical interferometer:1;anomaly detection:1;multi-agent trajectory:1;automated driving:1;depth estimation:1;anytime algorithm:1;energy-aware optimization:1;mobile device:1;reward design:1;zero-shot imitation learning:1;one-shot imitation learning:1;multi-task imitation:1;benchmarking:1;household activity:1;stacking:1;model-based planning:1;tree-search:1;shape completion:1;contact sensing:1;pre-trained model:1;vision language grounding:1;clip:1;learning for human-robot collaboration:1;point cloud registration:1;implicit shape model:1;equivariant neural network:1;multi-camera:1;3d object detection:1;out-of-distribution state:1;quadrotor:1;skill-based transfer learning:1;grasp planning:1;learning from observation:1;differentiable sound rendering:1;auditory scene analysis:1;motion planning:1;uncertainty estimation:1;kinematic:1;semantic scene graph:1;embodied exploration:1;learning for visual navigation:1;off-road navigation:1;terrain adaptation:1;equivariance:1;error detection:1;lifelong learning:1;long-horizon planning:1;adversarial:1;detection:1;multimodal:1;optical flow:1;bimanual manipulation:1;dynamic manipulation:1;reset-free:1;descriptor learning:1;object correspondence:1;q-learning:1;task planning:1;relational reinforcement learning:1;transfer learning:1;av testing:1;bayesian optimization:1;mat\u00e9rn kernel:1;6d robotic grasping:1;social navigation:1;group-based navigation:1;application of robot learning in navigation:1;object grasping and manipulation:1;multimodal fusion:1;intrinsic motivation:1;touch:1;curiosity:1;multi-robot learning:1;mobile navigation:1;rolling shutter correction:1;imu:1;learning:1;energy-based model:1;behavioral attribution:1;robot motion:1;human impression:1;non-stationarity:1;visual localization:1;camera pose regression:1;novel view synthesis:1;factor graph:1;energy-based learning:1;observation model:1;language for shared autonomy:1;language & robotic:1;learned latent action:1;language grounding:1;3d:1;referring task:1;language model:1;3d visual grounding:1;3d navigation:1;structured representation:1;autonomous assembly:1;temporal logic:1;trajectory planning:1;real-time verification:1;social robot:1;backchanneling:1;field robotic:1;latent model:1;reachability density distribution:1;learning density distribution:1;liouville theorem:1;camera calibration:1;learning from agent with different dynamic:1;inertial navigation:1;deep neural network:1;natural language:1;visuomotor manipulation:1;manipulation planning:1;signed distance field:1;implicit object representation:1;multi-stage imitation learning:1;multi-modality:1;ranking:1;trajectory optimization:1;learning from choice:1;trajectory prediction:1;regrasping:1;state estimation:1;invariant ekf:1;safety rl:1;model predict control:1;gait:1;biomechanic:1;energetic:1;robot:1;safe control:1;competitive human-robot interaction:1;joint trajectory prediction:1;motion prediction:1;preference learning:1;rearrangement task:1;domain randomization:1;likelihood-free inference:1;object-object affordance:1;vision for robotic:1;large-scale learning:1;object dataset:1;multisensory learning:1;implicit representation:1;fine-tuning:1;movement primitive:1;sampling-based motion planning:1;diffeomorphism:1;normalising flow:1;sampling distribution:1;prm:1;deep learning for robotic manipulation:1;learning for motion planning:1;semantic manipulation:1;probabilistic and geometric depth:1;monocular 3d detection:1;unseen object instance segmentation:1;tactile skin:1;magnetic sensing:1;soft sensor:1;evolution strategy:1;redundancy resolution:1;action bias:1;velocity control:1;cem:1;data-driven mpc:1;uncertainty:1;debugging and evaluation:1;algorithmic transparency:1;constrained markov decision process:1;safe exploration:1;rough terrain navigation:1;model uncertainty:1;stiffness control:1;spatial reasoning:1;model-predictive control:1;certified control:1;learning for control:1;multi-task reinforcement learning:1;depth completion:1;3d perception:1;data collection:1;continual learning:1;online learning:1;point cloud prediction:1;3d lidar:1;temporal convolutional network:1;relation:1;self-supervised reinforcement learning:1;off-road driving:1;perception:1;relocalization:1;localization:1;visual place recognition:1;sequence matching:1;contrastive learning:1;scene reconstruction:1;differentiable rendering:1;skill extraction:1;human preference:1;real world:1;episodic policy search:1;versatile skill learning:1;hierarchical rl:1;curriculum learning:1;stochastic policy:1;hierarchical optimization:1;audio perception:1;multi-modal learning:1;nonlinear optimization:1;constraint graph:1;robotic sequential manipulation:1;slam:1;dense mapping:1;multi-view stereo:1;robotic touch:1;disentanglement:1;shear:1;object reconstruction:1;tactile robotic:1;sim2real:1;out-of-distribution detection:1;generalization:1;pac-bayes:1;robot task planning:1;3d scene graph:1;learning to plan:1;multimodal perception:1;object state estimation:1;audio:1;fleet learning:1;human robot interaction:1;robotic introspection:1;bayesian deep learning:1;gaussian process:1;simulation:1;machine learning:1;analytical mechanic:1;variational inference:1;soft robot:1;force control:1;visual locomotion:1;visual representation:1;interactive perception:1;inverse reinforcement learning:1;conservative policy evaluation:1;simulation environment:1;virtual reality interface:1"
    }
}